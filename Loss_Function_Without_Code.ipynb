{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rajat504/CLOUDYML/blob/main/Loss_Function_Without_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNm6U0QX5cLD"
      },
      "source": [
        "# ALL about Loss function\n",
        "\n",
        "\n",
        "<img align=\"left\" width=\"300\" src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSI7eAfZ7O2tHjAOIeBpixRhJ_MI8s8B0u8KwpIgAremMxN3yE7yvFB95hDY7xHdLeMBxc&usqp=CAUg\">\n",
        "\n",
        "<img align=\"left\" width=\"200\"  src=\"https://storage.needpix.com/rsynced_images/emoji-2304720_1280.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnhoXVRt6x5O"
      },
      "source": [
        "# Table of contents\n",
        "1. What is loss Function?\n",
        "2. How to choose loss function?\n",
        "    1. Regression Problem  \n",
        "        1. Mean Squared Error\n",
        "        2. Mean Absolute Error Loss\n",
        "        3. Huber Loss (Smooth Mean Absolute Error)\n",
        "3. Binary Classification Problem   \n",
        "    1. Binary Cross Entropy Loss\n",
        "    2. Hinge Loss\n",
        "5. Multi-Class Classification Problem  \n",
        "    1. Categorical Cross Entropy\n",
        "    2. Sparse Categorical Cross Entropy\n",
        "    3. Kullback Leibler (KL) Divergence Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q7Xif6jzl-nr"
      },
      "outputs": [],
      "source": [
        "# import numpy, tensorflow, norm from scipy, matplotlib  libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from scipy.linalg import norm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHE-3pnm0G6I"
      },
      "source": [
        "# What is loss function?\n",
        "\n",
        "It is a method of evaluating how well your algorithm models your dataset. If your predictions are totally wrong, your loss function will output a higher number. If they are pretty good, it will output a lower number. As you change pieces of your algorithm to try and improve your model, your loss function will tell you if you are getting close to your goal or getting away."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfNmN6UH0sFj"
      },
      "source": [
        "# Note:\n",
        "\n",
        "A lot of the loss functions that you see implemented in machine learning can get complex and confusing. But if you remember the end goal of all loss functions—measuring how well your algorithm is doing on your dataset—you can keep that complexity in check."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0Ckw_lgCZJe"
      },
      "source": [
        "There are many functions that could be used to estimate the error of a set of weights in a neural network.\n",
        "\n",
        "We need a function that will reach to a ser of weights after each iteration. These weights will result in improved performance and lesser loss value.\n",
        "\n",
        "The loss function gets the weights based on historical training data.\n",
        "\n",
        "We have a training dataset with one or more input variables and we require a model to estimate model weight parameters that best map examples of the inputs to the output or target variable.\n",
        "\n",
        "Given input, the model is trying to make predictions that match the data distribution of the target variable. A loss function estimates how closely the distribution of predictions made by a model matches the distribution of target variables in the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxsirWxIDO7_"
      },
      "source": [
        "# Cross Entropy\n",
        "\n",
        "The error between two probability distributions is measured using cross-entropy.\n",
        "\n",
        "When modeling a classification problem where we are interested in mapping input variables to a class label, we can model the problem as predicting the probability of an example belonging to each class. In a binary classification problem, there would be two classes, so we may predict the probability of the example belonging to the first class. In the case of multiple-class classification, we can predict a probability for the example belonging to each of the classes.\n",
        "\n",
        "In the training dataset, the probability of an example belonging to a given class would be 1 or 0, as each sample in the training dataset is a known example from the domain. We know the answer.\n",
        "\n",
        "We would seek a set of model weights that minimize the difference between the model’s predicted probability distribution given the dataset and the distribution of probabilities in the training dataset. This is called the cross-entropy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwskedbaEA2S"
      },
      "source": [
        "# How to choose loss function?\n",
        "\n",
        "## Important\n",
        "The choice of loss function is directly related to the activation function used in the output layer of your neural network. These two design elements are connected.\n",
        "\n",
        "\n",
        "Think of the configuration of the output layer as a choice about the framing of your prediction problem, and the choice of the loss function as the way to calculate the error for a given framing of your problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TLlJyWNE3ja"
      },
      "source": [
        "# Regression Problem\n",
        "\n",
        "Regression is the task of predicting a continuous quantity. A regression algorithm may predict a discrete value, but the discrete value in the form of an integer quantity. Regression predictions can be evaluated using root mean squared error, whereas classification predictions cannot.\n",
        "\n",
        "# Note:\n",
        "For regression problem the output layer consist of one node with a linear activation unit.\n",
        "\n",
        "So, based on this we can select a loss function.\n",
        "The loss function for such case is Mean Squared error (MSE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9I3fHvVkkIKV"
      },
      "source": [
        "# Mean Squared Error\n",
        "\n",
        "The Mean Squared Error (MSE) is perhaps the simplest and most common loss function, often taught in introductory Machine Learning courses. To calculate the MSE, you take the difference between your model’s predictions and the ground truth, square it, and average it out across the whole dataset.\n",
        "\n",
        "The MSE will never be negative, since we are always squaring the errors. The MSE is formally defined by the following equation:\n",
        "\n",
        "![link text](https://cdn-media-1.freecodecamp.org/images/hmZydSW9YegiMVPWq2JBpOpai3CejzQpGkNG)\n",
        "\n",
        "The below code is a working implementation of a function for calculating the mean squared error for a list of acual and a list of predicted real-values quantities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "g9tHvRb4zwDn"
      },
      "outputs": [],
      "source": [
        "# define a function named mse with parameter y_true, y_pred\n",
        "def mse(y_true, y_pred):\n",
        "  # calculate the difference between y_true and y_pred\n",
        "\n",
        "  # return mean of (y_true - y_pred)^2\n",
        "  return np.mean(np.power(y_true - y_pred, 2))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42_g59qsl4Yz",
        "outputId": "16eb82cc-c8ad-486d-9558-5bbfd74ce327"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.1 0.2 0.3 0.4 0.5 0.6]\n",
            "(6,)\n",
            "[0.1 0.2 0.3 0.4 0.5 0.6]\n",
            "(6,)\n",
            "0.0\n"
          ]
        }
      ],
      "source": [
        "# create a array for true label of random values of any size\n",
        "y_true = np.array([0.1,0.2,0.3,0.4,0.5,0.6])\n",
        "print(y_true)\n",
        "print(y_true.shape)\n",
        "\n",
        "# create a array for predicted label same size as compared to true label but values can be same or different\n",
        "y_pred = np.array([0.1,0.2,0.3,0.4,0.5,0.6])\n",
        "print(y_pred)\n",
        "print(y_pred.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# create a array for predicted label same size as compared to true label but values can be same or different\n",
        "mse_result = mse(y_true, y_pred)\n",
        "\n",
        "# create a variable to store returned loss value from mse function\n",
        "\n",
        "\n",
        "\n",
        "# print the mse value\n",
        "print(mse_result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2icA4SYqoX0y"
      },
      "source": [
        "From the above output you can see that the error we got is 1.5 because there is one predicted/target value is not matching with actual/true label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZW1Ie01bxgXA"
      },
      "source": [
        "# Mean Absolute Error Loss\n",
        "\n",
        "Mean Absolute Error (MAE) is also another important loss function in regression problem. It is defined as the average of the absolute difference between the actual and predicted values.\n",
        "\n",
        "The formula for MAE:\n",
        "\n",
        "![link text](https://www.statisticshowto.com/wp-content/uploads/2016/10/MAE.png)\n",
        "\n",
        "\n",
        "MAE is more robust to outliers compared to MSE\n",
        "\n",
        "it is computationally expensive as modulus error is complex to solve compared to square error.\n",
        "\n",
        "The gradient becomes large even for small loss as gradient remains same during the process, which is not appropriate for learning. To fix this issues, dynamic learning rate can be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01ABfCjq6pXK",
        "outputId": "194e5da8-b744-4a2b-a9f7-a449f4830593"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 3.  -0.5  2.   7. ]\n",
            "(4,)\n",
            "[2.5 0.  2.  8. ]\n",
            "(4,)\n",
            "0.5\n"
          ]
        }
      ],
      "source": [
        "# import mean_absolute_error from sklearn library\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# create a array for true label of random values of any size\n",
        "y_true=np.array([3,-0.5,2,7])\n",
        "print(y_true)\n",
        "print(y_true.shape)\n",
        "\n",
        "# create a array for predicted label same size as compared to true label but values can be same or different\n",
        "y_pred=np.array([2.5,0.0,2,8])\n",
        "print(y_pred)\n",
        "print(y_pred.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# create a array for predicted label same size as compared to true label but values can be same or different\n",
        "\n",
        "\n",
        "# create a variable to store returned loss value from mean_absolute_error\n",
        "mae_result = mean_absolute_error(y_true, y_pred)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print the mae value\n",
        "print(mae_result)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# create a variable to store returned loss value from mean_absolute_error function\n",
        "\n",
        "\n",
        "# print mean absolute error value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFSm5j3YyNCH"
      },
      "source": [
        "# Huber Loss (Smooth Mean Absolute Error)\n",
        "\n",
        "Huber loss plays an important role by combining both MSE and MAE. It changes the quadratic equation to linear, if the loss is higher. If the error is less than cutoff (epsilon), MSE is used and otherwise MAE can be used.\n",
        "\n",
        "\n",
        "Formula for Huber Loss:\n",
        "\n",
        "![link text](https://camo.githubusercontent.com/dacfd2c868054faff72ba65a9e4eca85d07366e7cac1f2de83c970cadb66525d/68747470733a2f2f63646e2e616e616c79746963737669646879612e636f6d2f77702d636f6e74656e742f75706c6f6164732f323031392f30382f68756265722e6a70672e6a7067)\n",
        "\n",
        "\n",
        "Huber loss curves around the minima which decreases the gradient, which is better compared to MAE as MAE has a constantly large gradient. This leads to missing minima at the end of training using gradient descent. On the other hand, Huber loss is less sensitive to outliers compared to the mean squared error loss.\n",
        "\n",
        "Please note, the choice of delta is important as it helps to determines the an outlier criteria.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0pzIpnB6KZG",
        "outputId": "1250299c-bfbc-475d-fd19-0d329f7e108c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.1 1. ]\n",
            " [1.  1. ]\n",
            " [1.  1. ]]\n",
            "(3, 2)\n",
            "[[ 0.1  1. ]\n",
            " [ 1.   1. ]\n",
            " [-2.  -2. ]]\n",
            "(3, 2)\n",
            "tf.Tensor(0.8333333, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "# create a list for true label of random values of any size\n",
        "y_true = np.array([[0.1 ,1.], [1. ,1.], [1.,1.]])\n",
        "print(y_true)\n",
        "print(y_true.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# create a list for predicted label same size as compared to true label but values can be same or different\n",
        "y_pred = np.array([[0.1 ,1.], [1. ,1.],  [-2. , -2.]])\n",
        "print(y_pred)\n",
        "print(y_pred.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# create a variable to store huber loss resturned by tendorflow with delta = 1.0\n",
        "huber_loss = tf.keras.losses.Huber(delta=1.0)\n",
        "\n",
        "\n",
        "# create a variable to store huber_loss loss resturned by tensorflow\n",
        "huber_loss_result = huber_loss(y_true, y_pred)\n",
        "\n",
        "\n",
        "\n",
        "# print huber loss value\n",
        "print(huber_loss_result)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dK0JRd-BE9Lu"
      },
      "source": [
        "# Binary Classification Problem\n",
        "\n",
        "Binary classification is the simplest kind of machine learning problem. The goal of binary classification is to categorise data points into one of two buckets: 0 or 1, true or false, to survive or not to survive, blue or no blue eyes, etc.\n",
        "\n",
        "# Note:\n",
        "\n",
        "For classification problem the output layer consist of one node with a sigmoid activation unit or softmax.\n",
        "\n",
        "So, based on this we can select a loss function.\n",
        "The loss function for such case is Cross-Entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKmX3-MFsK4V"
      },
      "source": [
        "# Binary Cross Entropy Loss\n",
        "\n",
        "Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. So predicting a probability of 0.2 when the actual observation label is 1 would be bad and result in a high loss value. A perfect model would have a log loss of 0.\n",
        "\n",
        "Formula for binary cross entropy:\n",
        "![link text](https://cdn.analyticsvidhya.com/wp-content/uploads/2021/03/Screenshot-from-2021-03-03-11-33-29.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zv8xtaWBu4d2",
        "outputId": "afaf684f-9fa8-4375-8592-d30c4c7909e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 0 0]\n",
            "(4,)\n",
            "[0.6 0.3 0.2 0.8]\n",
            "(4,)\n",
            "0.9882113\n"
          ]
        }
      ],
      "source": [
        "# create a list for true label of random values of any size\n",
        "y_true = np.array([0,1,0,0])\n",
        "print(y_true)\n",
        "print(y_true.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# create a list for predicted label same size as compared to true label but values can be same or different\n",
        "y_pred = np.array([0.6 ,0.3, 0.2 ,0.8])\n",
        "print(y_pred)\n",
        "print(y_pred.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# create a variable named bin_cross_entropy which is a constructor of BinaryCrossentropy from tensorflow\n",
        "bin_cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "\n",
        "\n",
        "# create a variable to store bin_cross_entropy loss resturned by tensorflow\n",
        "bin_cross_entropy_value = bin_cross_entropy(y_true, y_pred).numpy()\n",
        "\n",
        "\n",
        "\n",
        "# print bin_cross_entropy_value\n",
        "print(bin_cross_entropy_value)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwiP20d10J71"
      },
      "source": [
        "# Hinge Loss\n",
        "\n",
        "Hinge Loss is another loss function for binary classification problems. It is primarily developed for Support Vector Machine (SVM) models. The hinge loss is calculated based on “maximum-margin” classification.\n",
        "\n",
        "This loss function is used if the target values are in the set (-1, 1). The target variable must be modified to have values in the set (-1, 1), which means if y has value as 0, it needs to be changed as -1.\n",
        "\n",
        "Formula for Hinge Loss:\n",
        "\n",
        "![link text](https://www.machinecurve.com/wp-content/uploads/2019/10/image-1.png)\n",
        "\n",
        "The hinge loss function tries to ensure the correct sign by assigning more error if there is a difference in the sign between the actual and predicted class values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tyn_hOw30ziK",
        "outputId": "6f6d982a-9132-42c2-8458-f110852659f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.1 1. ]\n",
            " [0.  0. ]]\n",
            "(2, 2)\n",
            "[[0.6 0.4]\n",
            " [0.4 0.6]]\n",
            "(2, 2)\n",
            "0.885\n"
          ]
        }
      ],
      "source": [
        "# create a list for true label of random values of any size\n",
        "y_true = np.array([[0.1,1.],[0.,0.]])\n",
        "print(y_true)\n",
        "print(y_true.shape)\n",
        "# create a list for predicted label same size as compared to true label but values can be same or different\n",
        "y_pred = np.array( [[0.6, 0.4],[0.4 ,0.6]])\n",
        "print(y_pred)\n",
        "print(y_pred.shape)\n",
        "\n",
        "# create a variable named hinge_loss which is a constructor of Hinge from tensorflow\n",
        "hinge_loss = tf.keras.losses.Hinge()\n",
        "\n",
        "\n",
        "# create a variable to store hinge_loss loss returned by tensorflow\n",
        "hinge_loss_value = hinge_loss(y_true, y_pred).numpy()\n",
        "\n",
        "\n",
        "# print hinge_loss value\n",
        "print(hinge_loss_value)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ni8iMFo0FBP2"
      },
      "source": [
        "# Multi-Class Classification Problem\n",
        "\n",
        "A classification problem including more than two classes is called Multi-Class classification\n",
        "\n",
        "example: classify a set of flowers which may be roses, lotus, lily, etc.\n",
        "\n",
        "Multi-class classification makes the assumption that each sample is assigned to one and only one label\n",
        "\n",
        "A flower can be either rose or lotus but not both simultaneously.\n",
        "\n",
        "such as converting the N number of classes to N number binary columns representing each class. By doing so, we can use a binary classifier for Multi Classification problems.\n",
        "\n",
        "The cross-entropy is then summed across each binary feature and averaged across all examples in the dataset.\n",
        "\n",
        "Formula for using binary cross entropy for multi-class classification problem.\n",
        "\n",
        "![link text](https://cdn.analyticsvidhya.com/wp-content/uploads/2021/03/Screenshot-from-2021-03-03-11-43-42.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ugCslgauKQz"
      },
      "source": [
        "# Categorical Cross Entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTLLo_fDsoVw",
        "outputId": "b784b5c2-5683-4d43-88c4-404617cccddf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 1 0]\n",
            " [0 0 1]]\n",
            "(2, 3)\n",
            "[[0.05 0.95 0.  ]\n",
            " [0.1  0.8  0.1 ]]\n",
            "(2, 3)\n",
            "1.1769392\n"
          ]
        }
      ],
      "source": [
        "# create a list for true label of random values of any size\n",
        "y_true = np.array([[0, 1, 0],[0, 0, 1]])\n",
        "print(y_true)\n",
        "print(y_true.shape)\n",
        "\n",
        "# create a list for predicted label same size as compared to true label but values are probability distribution for example: [0.2,0.4,0.4]\n",
        "y_pred = np.array([[0.05 ,0.95, 0 ],[0.1 , 0.8 , 0.1 ]])\n",
        "print(y_pred)\n",
        "print(y_pred.shape)\n",
        "\n",
        "# create a variable named cat_cross_entropy which is a constructor of CategoricalCrossentropy from tensorflow\n",
        "cat_cross_entropy = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "\n",
        "# create a variable to store cat_cross_entropy loss returned by tensorflow\n",
        "cat_cross_entropy_value = cat_cross_entropy(y_true, y_pred).numpy()\n",
        "\n",
        "\n",
        "# print cat_cross_entropy_value\n",
        "print(cat_cross_entropy_value)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHpVb4Ajs-sv"
      },
      "source": [
        "# Sparse Categorical Cross Entropy\n",
        "\n",
        "As seen in categorical cross entropy we convert the class/labels into binary i.e one hot encode.\n",
        "then we find the loss.\n",
        "\n",
        "but what if we feed data with class/ label as integer.\n",
        "\n",
        "For this we have sparse categorical cross entropy function.\n",
        "\n",
        "\n",
        "Advantage compared Multi-class Cross Entropy: Above example shows that for multi-class cross entropy, the target needs a one hot encoded vector which contains a lot of zero values, leading to significant memory requirement. By using sparse categorical cross entropy, one can save computation time with lower memory requirement because it only requires a single integer for a class, rather than a whole vector.\n",
        "\n",
        "Disadvantage of Sparse Multi-class Cross Entropy: Multi-class cross entropy can be used in any kind of classification problem. Whereas, Sparse categorical cross entropy can only be used when each input belongs to a single class only.\n",
        "\n",
        "\n",
        "\n",
        "# Note:\n",
        "\n",
        "The ground truth label(y_true) shape = [batch_size, d0, .. dN-1, 1].\n",
        "\n",
        "The predicted label(y_pred) shape = [batch_size, d0, .. dN]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAVgXAMDsoTy",
        "outputId": "99d31c85-5c6b-4976-b233-8f9ff503e79f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 2]\n",
            "(2,)\n",
            "[[0.05 0.95 0.  ]\n",
            " [0.2  0.8  0.  ]]\n",
            "(2, 3)\n",
            "8.084695\n"
          ]
        }
      ],
      "source": [
        "# create a array for true label of random integer values of any size\n",
        "y_true = np.array([1,2])\n",
        "print(y_true)\n",
        "print(y_true.shape)\n",
        "\n",
        "# create a array of shape of rows similar to number of classes\n",
        "y_pred = np.array([[0.05,0.95,0],[0.2,0.8,0]])\n",
        "print(y_pred)\n",
        "print(y_pred.shape)\n",
        "\n",
        "\n",
        "\n",
        "# create a variable named sparse_cat_cross_entropy which is a constructor of Hinge from tensorflow\n",
        "sparse_cat_cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "\n",
        "# create a variable to store sparse_cat_cross_entropy loss resturned by tensorflow\n",
        "sparse_cat_cross_entropy_value = sparse_cat_cross_entropy(y_true, y_pred).numpy()\n",
        "\n",
        "\n",
        "# print sparse_cat_cross_entropy_value\n",
        "print(sparse_cat_cross_entropy_value)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QgaKYE41BeK"
      },
      "source": [
        "# Kullback Leibler (KL) Divergence Loss\n",
        "\n",
        "Kullback Leibler Divergence is a measure that shows how much two probability distributions are different from each other.\n",
        "\n",
        "Formula for Kullback Leibler (KL) Divergence Loss:\n",
        "\n",
        "![link text](https://miro.medium.com/max/810/0*opyFpDwDt0H8rfCv)\n",
        "\n",
        "\n",
        "KL divergence loss of 0 suggests the distributions are identical.\n",
        "\n",
        "KL Divergence is somehow similar to cross-entropy. Like multi-class cross entropy, here also actual targets (y) needs to be one-hot encoded. It calculates how much information is lost if the predicted probability distribution is used to approximate the desired target probability distribution.\n",
        "\n",
        "KL divergence is mostly used in Variational Autoencoders. Here, the autoencoders learn how to encode the samples into a latent probability distribution, which is further fed into a decoder for generating output. Additionally, KL divergence can be used in multiclass classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyxgY5Ei4z7X",
        "outputId": "2667a40d-cb2c-4a44-9c9b-d3bbb881f934"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 1]\n",
            " [0 0]]\n",
            "(2, 2)\n",
            "[[0.6 0.4]\n",
            " [0.4 0.6]]\n",
            "(2, 2)\n",
            "0.45814306\n"
          ]
        }
      ],
      "source": [
        "# create a list for true label of random values of any size\n",
        "y_true = np.array([[0 ,1],[0 ,0]])\n",
        "print(y_true)\n",
        "print(y_true.shape)\n",
        "\n",
        "# create a list for predicted label same size as compared to true label but values are probability distribution for example: [[0.6,0.4],[0.4,0.6]]\n",
        "y_pred = np.array([[0.6 ,0.4],[0.4 ,0.6]])\n",
        "print(y_pred)\n",
        "print(y_pred.shape)\n",
        "\n",
        "\n",
        "\n",
        "# create a variable named kl which is a constructor of Hinge from tensorflow\n",
        "kl = tf.keras.losses.KLDivergence()\n",
        "\n",
        "\n",
        "# create a variable to store Kl_value loss resturned by tensorflow\n",
        "kl_value = kl(y_true, y_pred).numpy()\n",
        "\n",
        "\n",
        "# print kl_value\n",
        "print(kl_value)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwAjXP2k8Xm8"
      },
      "source": [
        "# Assignment Summary\n",
        "\n",
        "When creating a neural network we must select a apropriate loss function according to activation function used:\n",
        "\n",
        "### following are the loss function discussed in this assignment\n",
        "\n",
        "For Regression Problem:\n",
        "1. Mean Squared Error\n",
        "2. Mean Absolute Error Loss\n",
        "3. Huber Loss (Smooth Mean Absolute Error)\n",
        "\n",
        "For Binary CLassification Problem:\n",
        "1. Binary Cross Entropy Loss\n",
        "2. Hinge Loss\n",
        "\n",
        "For Multi-Class CLassification Problem:\n",
        "1. Categorical Cross Entropy\n",
        "2. Sparse Categorical Cross Entropy\n",
        "3. Kullback Leibler (KL) Divergence Loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83689bdn6aP6"
      },
      "source": [
        "![link text](https://image.shutterstock.com/image-vector/congratulations-greeting-sign-graduation-party-260nw-1396729610.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCzz6fTl8SA1"
      },
      "source": [
        "# we have learned all the loss function used in neural network and their properties with code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAPG03FIEPtq"
      },
      "source": [
        "# Please fill the below feedback form about this assignment\n",
        "\n",
        "https://forms.zohopublic.in/cloudyml/form/CloudyMLDeepLearningFeedbackForm/formperma/VCFbldnXAnbcgAIl0lWv2blgHdSldheO4RfktMdgK7s"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}